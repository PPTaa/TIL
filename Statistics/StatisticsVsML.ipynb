{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. regression\n",
    "\n",
    "Set : $y = X \\cdot w + b + \\epsilon = X \\cdot w + \\epsilon$\n",
    "- assume for 1 observative : x = (1, x), w = (b, w)\n",
    "\n",
    "We know:  (here, v and X is constant about w)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\cfrac {\\partial}{\\partial w} \\left( v^T \\cdot w \\right) &= v \\\\\n",
    "\\cfrac {\\partial}{\\partial w} \\left( w^T \\cdot v \\right) &= v \\\\\n",
    "(w^T \\cdot X)^T &= X^T \\cdot w \\\\\n",
    "\\cfrac {\\partial}{\\partial w} \\left( w^T \\cdot X \\cdot w \\right) &= X \\cdot w + X^T \\cdot w\n",
    "\\end{align}$$\n",
    "\n",
    "So, We solve: y.shape = (N, ), X.shape=(N, p+1), w.shape = (p+1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "y &= X \\cdot w + \\epsilon = \\hat y + \\epsilon\\\\\n",
    "Loss(w) &= \\epsilon ^2 =||y - \\hat y||^2 = (y - X \\cdot w)^T \\cdot (y - X \\cdot w)\\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= 2 (X^T \\cdot X) \\cdot w - 2X^T \\cdot y = 0 \n",
    "\\end{align}$$\n",
    "\n",
    "We get $\\hat w$ :\n",
    "$$ \\left| {\\cfrac {\\partial Loss}{\\partial w}} \\right|_{w = \\hat w} = 0 \\Rightarrow \n",
    "\\hat w = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y $$\n",
    "\n",
    "### 1.1 Using statistical tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "bunch = load_iris()\n",
    "bunch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\ai\\\\python\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunch.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.DataFrame(bunch.data, columns=bunch.feature_names)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      "sl    150 non-null float64\n",
      "sw    150 non-null float64\n",
      "pl    150 non-null float64\n",
      "pw    150 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "iris.columns = ['sl', 'sw', 'pl', 'pw']\n",
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "petal width를 target으로 회귀식을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sl + sw + pl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' + '.join(iris.columns[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pw ~ sl + sw + pl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formula = 'y ~ x1 + x2 + x3'\n",
    "features = iris.columns\n",
    "formula = '%s ~ '%iris.columns[3]\n",
    "formula += ' + '.join(iris.columns[:3])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>pw</td>        <th>  R-squared:         </th> <td>   0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   734.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 24 Dec 2019</td> <th>  Prob (F-statistic):</th> <td>7.83e-88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:18:20</td>     <th>  Log-Likelihood:    </th> <td>  36.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   150</td>      <th>  AIC:               </th> <td>  -65.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   146</td>      <th>  BIC:               </th> <td>  -53.46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2403</td> <td>    0.178</td> <td>   -1.347</td> <td> 0.180</td> <td>   -0.593</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sl</th>        <td>   -0.2073</td> <td>    0.048</td> <td>   -4.363</td> <td> 0.000</td> <td>   -0.301</td> <td>   -0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sw</th>        <td>    0.2228</td> <td>    0.049</td> <td>    4.553</td> <td> 0.000</td> <td>    0.126</td> <td>    0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pl</th>        <td>    0.5241</td> <td>    0.024</td> <td>   21.399</td> <td> 0.000</td> <td>    0.476</td> <td>    0.572</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.609</td> <th>  Durbin-Watson:     </th> <td>   1.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.061</td> <th>  Jarque-Bera (JB):  </th> <td>   6.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.223</td> <th>  Prob(JB):          </th> <td>  0.0332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.944</td> <th>  Cond. No.          </th> <td>    90.1</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     pw   R-squared:                       0.938\n",
       "Model:                            OLS   Adj. R-squared:                  0.937\n",
       "Method:                 Least Squares   F-statistic:                     734.4\n",
       "Date:                Tue, 24 Dec 2019   Prob (F-statistic):           7.83e-88\n",
       "Time:                        15:18:20   Log-Likelihood:                 36.751\n",
       "No. Observations:                 150   AIC:                            -65.50\n",
       "Df Residuals:                     146   BIC:                            -53.46\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2403      0.178     -1.347      0.180      -0.593       0.112\n",
       "sl            -0.2073      0.048     -4.363      0.000      -0.301      -0.113\n",
       "sw             0.2228      0.049      4.553      0.000       0.126       0.320\n",
       "pl             0.5241      0.024     21.399      0.000       0.476       0.572\n",
       "==============================================================================\n",
       "Omnibus:                        5.609   Durbin-Watson:                   1.573\n",
       "Prob(Omnibus):                  0.061   Jarque-Bera (JB):                6.811\n",
       "Skew:                           0.223   Prob(JB):                       0.0332\n",
       "Kurtosis:                       3.944   Cond. No.                         90.1\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(formula = formula, data = iris)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -0.240307\n",
       "sl          -0.207266\n",
       "sw           0.222829\n",
       "pl           0.524083\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using numpy with linear algebra\n",
    "Now, we'll compute with numpy :\n",
    "$$ \\hat w =  (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, _ = load_iris(return_X_y = True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set : $y = X \\cdot w + b + \\epsilon = X \\cdot w + \\epsilon$\n",
    "- assume: x = (1, x), w = (b, w). So,\n",
    "- y = X[:, 3]\n",
    "- X = np.hstack(np.ones(shape), X[:, :3])\n",
    "\n",
    "And then, you can use numpy.linalg.inv for inverse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150,), (150, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X[:, 3]\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X[:, :3]))\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat w =  (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X's covariant's inverse\n",
    "from numpy.linalg import inv\n",
    "invCov_X = inv(X.T.dot(X))\n",
    "w = invCov_X.dot(X.T).dot(y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Machine Learning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000 # 50000\n",
    "batch = 16\n",
    "lr = 0.0001 # 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        y_hat = x_batch.dot(w)\n",
    "        loss += (y_batch - y_hat).dot((y_batch - y_hat))\n",
    "        dw = X.T.dot(X).dot(w) - X.T.dot(y) # = d Loss(w) / d(w)\n",
    "        w -= lr*dw\n",
    "        \n",
    "    loss /= rows\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "Loss(w) &= (y - X \\cdot w)^T \\cdot (y - X \\cdot w)\\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= 2 (X^T \\cdot X) \\cdot w - 2X^T \\cdot y \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24030739, -0.20726607,  0.22282854,  0.52408311])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1926d5e6d30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#matplotlib inline\n",
    "loss_df = pd.DataFrame(losses)\n",
    "loss_df[:100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1926d70e2b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df[10000:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. classification\n",
    "We consider binary classification, and want to predict not directly <font color='red'>y</font> but <font color='blue'>p(y=1)</font>.\n",
    "\n",
    "We Set:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat y &= P(y=1) = \\sigma(X \\cdot w) \\\\\n",
    "\\sigma(z) &= \\cfrac 1 {1+\\exp(-z)}\\\\\n",
    "\\cfrac {\\partial \\sigma(z)}{\\partial z} &= \\cfrac {\\exp(-z)} {(1+\\exp(-z))^2} = \\sigma(z)(1-\\sigma(z))\\\\\n",
    "Loss(w) &= NLL(w) \n",
    "= - \\left[ y^T \\cdot \\log P(y=1) \\right] - \\left[ (1-y)^T \\cdot \\log (1-P(y=1)) \\right] \\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= - X^T \\cdot \\left[ y (1 - P(y=1)) \\right] + X^T \\cdot \\left[(1-y) P(y=1) \\right] \\\\\n",
    "&= X^T \\cdot (\\hat y - y)\n",
    "\\end{align}$$\n",
    "\n",
    "### 2.1 statistical tool\n",
    "binary case만을 고려하기 위해, 첫 100개 항만 가져오도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.31629979,  0.70803893, -2.29291305,  0.28220666],\n",
       "       [-0.67690095, -0.77936232, -1.12511459, -0.85295128],\n",
       "       [ 2.70496343,  1.81256131,  0.09005406,  0.62008331],\n",
       "       [ 0.5868326 ,  0.36681905, -0.45731792, -0.46723319],\n",
       "       [ 0.65746416, -1.15725619, -0.03396529, -0.40039299]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=4, n_redundant=0, n_informative=1,\n",
    "                           n_clusters_per_class=1, random_state=4)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.31629979,  0.70803893, -2.29291305,  0.28220666],\n",
       "       [ 1.        , -0.67690095, -0.77936232, -1.12511459, -0.85295128],\n",
       "       [ 1.        ,  2.70496343,  1.81256131,  0.09005406,  0.62008331],\n",
       "       [ 1.        ,  0.5868326 ,  0.36681905, -0.45731792, -0.46723319],\n",
       "       [ 1.        ,  0.65746416, -1.15725619, -0.03396529, -0.40039299]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.124262\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>   100</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    95</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 24 Dec 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.8207</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>15:20:01</td>     <th>  Log-Likelihood:    </th> <td> -12.426</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -69.295</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.161e-23</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3037</td> <td>    0.603</td> <td>    0.503</td> <td> 0.615</td> <td>   -0.879</td> <td>    1.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -1.0410</td> <td>    0.731</td> <td>   -1.423</td> <td> 0.155</td> <td>   -2.474</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    5.6889</td> <td>    1.538</td> <td>    3.700</td> <td> 0.000</td> <td>    2.675</td> <td>    8.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.2788</td> <td>    0.553</td> <td>   -0.504</td> <td> 0.614</td> <td>   -1.363</td> <td>    0.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.8798</td> <td>    0.659</td> <td>   -1.336</td> <td> 0.182</td> <td>   -2.171</td> <td>    0.411</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.16 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  100\n",
       "Model:                          Logit   Df Residuals:                       95\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Tue, 24 Dec 2019   Pseudo R-squ.:                  0.8207\n",
       "Time:                        15:20:01   Log-Likelihood:                -12.426\n",
       "converged:                       True   LL-Null:                       -69.295\n",
       "                                        LLR p-value:                 1.161e-23\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3037      0.603      0.503      0.615      -0.879       1.486\n",
       "x1            -1.0410      0.731     -1.423      0.155      -2.474       0.392\n",
       "x2             5.6889      1.538      3.700      0.000       2.675       8.703\n",
       "x3            -0.2788      0.553     -0.504      0.614      -1.363       0.805\n",
       "x4            -0.8798      0.659     -1.336      0.182      -2.171       0.411\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.16 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using numpy with linear algebra\n",
    "can't get exact solution, but approximation method with Hessian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Machine Learning Method\n",
    "#### 1) SGD(확률적 경사하강법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 150000 # 50000\n",
    "batch = 34 # 2^n이 좋으나 데이터가 100개라 1/3로 함\n",
    "lrs = [0.0005, 0.00005, 0.00001]\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    epochLoss = 0\n",
    "    if epoch < 50000:\n",
    "        lr = lrs[0]\n",
    "    elif epoch < 100000:\n",
    "        lr = lrs[1]\n",
    "    else: lr = lrs[2]\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        # 아래는 직접 구현해보세요.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\hat y &= P(y=1) = \\sigma(X \\cdot w) \\\\\n",
    "Loss(w) &= NLL(w) \n",
    "= - \\left[ y^T \\cdot \\log P(y=1) \\right] - \\left[ (1-y)^T \\cdot \\log (1-P(y=1)) \\right] \\\\\n",
    "\\cfrac {\\partial Loss}{\\partial w} &= X^T \\cdot (\\hat y - y)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 모멘텀 기법 사용하기\n",
    "<참고> https://twinw.tistory.com/247\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99A14F455B0CF54C21)\n",
    "\n",
    "```\n",
    "v(t+1) = m * v(t) - a * dW(t)\n",
    "W(t+1) = W(t) + v(t+1)\n",
    "\n",
    "v(0) = 0, m = 0.9\n",
    "v(1) = - a * dW(0)\n",
    "W(1) = W(0) + v(1) = W(0) - a * dW(0)\n",
    "\n",
    "v(2) = m * v(1) - a * dW(1) = - 0.9 * a * dW(0) - a * dW(1)\n",
    "W(2) = W(1) - a * [ 0.9 * dW(0) + dW(1) ]\n",
    "\n",
    "v(3) = m * v(2) - a * dW(2) = - a * [ 0.9 * 0.9 * dW(0) + 0.9 * dW(1) + dW(2) ]\n",
    "W(3) = W(2) + v(3) = W(2) - a * [ 0.9 * 0.9 * dW(0) + 0.9 * dW(1) + dW(2) ]\n",
    "```\n",
    "- a: learning rate\n",
    "- m: momentum. memory of prior velocity. generally 0.9 ~ 0.99\n",
    "- v: velocity. moving speed and direction.\n",
    "\n",
    "```python\n",
    "v = m * v - learning_rate * dW\n",
    "W += v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19646919, -0.21386067, -0.27314855,  0.05131477,  0.21946897])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(123)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "# lrs = [0.0005, 0.00005, 0.00001]\n",
    "lr = 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = 0\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    epochLoss = 0\n",
    "#     if epoch < 50000:\n",
    "#         lr = lrs[0]\n",
    "#     elif epoch < 100000:\n",
    "#         lr = lrs[1]\n",
    "#     else: lr = lrs[2]\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19646919, -0.21386067, -0.27314855,  0.05131477,  0.21946897])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Nesterov 기법 사용하기\n",
    "![](https://t1.daumcdn.net/cfile/tistory/996E494B5B0D03A003)\n",
    "\n",
    "```python\n",
    "v = m * v - learning_rate * d(w + m*v)\n",
    "weight += v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46903266, -0.24571793,  0.41240044,  0.4701742 , -0.25338022])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "# lrs = [0.0005, 0.00005, 0.00001]\n",
    "lr = 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = 0\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    epochLoss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        y_hat_batch = sigma(x_batch.dot(w))\n",
    "        loss_batch = - y_batch.T.dot(np.log(y_hat_batch))\\\n",
    "                     - (1 - y_batch).T.dot(np.log(1 - y_hat_batch))\n",
    "        epochLoss += loss_batch\n",
    "        dw = x_batch.T.dot((y_hat_batch - y_batch))\n",
    "        y_hat_batch_next = sigma(x_batch.dot(w + m*v))\n",
    "        dw_next = x_batch.T.dot((y_hat_batch_next - y_batch))\n",
    "        v = m*v - lr*dw_next\n",
    "        w += v\n",
    "    epochLoss /= rows\n",
    "    losses.append(epochLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30365319, -1.04079873,  5.68893127, -0.27867004, -0.88003032])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) RMSprop 사용하기\n",
    "```python\n",
    "cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import shuffle, rand\n",
    "np.random.seed(234)\n",
    "w = rand(X.shape[-1]) - 0.5\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50000 # 50000\n",
    "batch = 34\n",
    "# lrs = [0.0005, 0.00005, 0.00001]\n",
    "lr = 0.00005\n",
    "rows = X.shape[0]\n",
    "losses = []\n",
    "randRow = np.arange(rows)\n",
    "m = 0.95\n",
    "v = 0\n",
    "cache=np.zeros(w.size)\n",
    "decay_rate=0.99\n",
    "eps=1e-8\n",
    "\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # for each epoch, shuffle X, y\n",
    "    shuffle(randRow)\n",
    "    epochLoss = 0\n",
    "    \n",
    "    for i in range(0, rows, batch):\n",
    "        batch_index = randRow[i:i+batch]\n",
    "        x_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18226938, -0.81510438,  5.04788791, -0.18684718, -0.70703686])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30366162, -1.04095437,  5.68894902, -0.27882206, -0.87983546])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
